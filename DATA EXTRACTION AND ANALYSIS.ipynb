{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4c77b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 95\n",
      "neg_score: 99\n",
      "polarity: -0.020618556594749708\n",
      "subjectivity: 0.2493573261576384\n",
      "avg_sent_length: 19.77227722772277\n",
      "percentage_complex: 0.3024536805207812\n",
      "fog_index: 8.02989236329742\n",
      "avg_no_of_words: 19.77227722772277\n",
      "complex_words: 604\n",
      "word_count: 1141\n",
      "syllable_count_per_word: 1.7871807711567351\n",
      "personal_pronouns: 6\n",
      "avg_word_length: 6.368573079145003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#extract  text data from url\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "url = \"https://insights.blackcoffer.com/which-one-is-better-ai-or-big-data/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "\n",
    "#save text to files as their ID names\n",
    "file = open('31.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "#tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "file_content = open(\"31.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#importing stopword lists joined together in one file to clean text\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "#sentimental analysis - importing negative and positive texts to use for sentiment analysis\n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "\n",
    "#calculations\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "import re\n",
    "#define a function to return the count of words with more than two syllables using regex\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "#cleaning text with stopwords from nltk package\n",
    "from nltk.corpus import stopwords\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "#remove unwanted punctuations        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "#count vowels in text\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "#find personal pronouns in text\n",
    "pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "pronouns = pronounRegex.findall(open(\"31.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex:\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7714c8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 88\n",
      "neg_score: 129\n",
      "polarity: -0.18894009129520695\n",
      "subjectivity: 0.24327354232816867\n",
      "avg_sent_length: 26.12987012987013\n",
      "percentage_complex 0.3220675944333996\n",
      "fog_index: 10.580775089721413\n",
      "avg_no_of_words: 26.12987012987013\n",
      "complex_words: 648\n",
      "word_count: 1217\n",
      "syllable_count_per_word: 1.9413518886679921\n",
      "personal_pronouns: 1\n",
      "avg_word_length: 6.803910614525139\n"
     ]
    }
   ],
   "source": [
    "#repeat procedure for others\n",
    "url = \"https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('37.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"37.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"37.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54e7881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 82\n",
      "neg_score: 80\n",
      "polarity: 0.012345678936137784\n",
      "subjectivity: 0.31764705820069206\n",
      "avg_sent_length: 20.525\n",
      "percentage_complex 0.25091352009744217\n",
      "fog_index: 8.310365408038976\n",
      "avg_no_of_words: 20.525\n",
      "complex_words: 412\n",
      "word_count: 825\n",
      "syllable_count_per_word: 1.6144945188794153\n",
      "personal_pronouns: 7\n",
      "avg_word_length: 6.010630758327427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('38.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"38.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"38.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb15428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 95\n",
      "neg_score: 109\n",
      "polarity: -0.06862745064398308\n",
      "subjectivity: 0.27163781588330516\n",
      "avg_sent_length: 22.55294117647059\n",
      "percentage_complex 0.3249869587897757\n",
      "fog_index: 9.151171254104147\n",
      "avg_no_of_words: 22.55294117647059\n",
      "complex_words: 623\n",
      "word_count: 1088\n",
      "syllable_count_per_word: 1.8737610850286908\n",
      "personal_pronouns: 3\n",
      "avg_word_length: 6.541764705882353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/what-jobs-will-robots-take-from-humans-in-the-future/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('39.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"39.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"39.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9940e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 114\n",
      "neg_score: 91\n",
      "polarity: 0.11219512140392623\n",
      "subjectivity: 0.3609154923223319\n",
      "avg_sent_length: 19.28421052631579\n",
      "percentage_complex 0.24563318777292575\n",
      "fog_index: 7.811937485635486\n",
      "avg_no_of_words: 19.28421052631579\n",
      "complex_words: 450\n",
      "word_count: 965\n",
      "syllable_count_per_word: 1.6975982532751093\n",
      "personal_pronouns: 18\n",
      "avg_word_length: 5.993333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/will-machine-replace-the-human-in-the-future-of-work/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('40.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"40.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"40.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54d09571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 91\n",
      "neg_score: 77\n",
      "polarity: 0.08333333283730159\n",
      "subjectivity: 0.239316238975333\n",
      "avg_sent_length: 25.050632911392405\n",
      "percentage_complex 0.28044466902476\n",
      "fog_index: 10.132431032166867\n",
      "avg_no_of_words: 25.050632911392405\n",
      "complex_words: 555\n",
      "word_count: 1091\n",
      "syllable_count_per_word: 1.6998484082870136\n",
      "personal_pronouns: 16\n",
      "avg_word_length: 6.260494537090282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/will-ai-replace-us-or-work-with-us/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('41.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"41.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"41.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a76896c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 72\n",
      "neg_score: 69\n",
      "polarity: 0.02127659559378301\n",
      "subjectivity: 0.2759295493621731\n",
      "avg_sent_length: 23.296875\n",
      "percentage_complex 0.278336686787391\n",
      "fog_index: 9.430084674714957\n",
      "avg_no_of_words: 23.296875\n",
      "complex_words: 415\n",
      "word_count: 787\n",
      "syllable_count_per_word: 1.6740442655935615\n",
      "personal_pronouns: 21\n",
      "avg_word_length: 6.183641975308642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/man-and-machines-together-machines-are-more-diligent-than-humans-blackcoffe/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('42.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"42.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"42.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53fc40a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 25\n",
      "neg_score: 28\n",
      "polarity: -0.056603772516909956\n",
      "subjectivity: 0.18088737139628883\n",
      "avg_sent_length: 18.68888888888889\n",
      "percentage_complex 0.28299643281807374\n",
      "fog_index: 7.588754128682786\n",
      "avg_no_of_words: 18.68888888888889\n",
      "complex_words: 238\n",
      "word_count: 464\n",
      "syllable_count_per_word: 1.6837098692033294\n",
      "personal_pronouns: 7\n",
      "avg_word_length: 6.235374149659864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('43.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"43.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"43.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2827d51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 36\n",
      "neg_score: 51\n",
      "polarity: -0.17241379112168057\n",
      "subjectivity: 0.30313588744551956\n",
      "avg_sent_length: 22.805555555555557\n",
      "percentage_complex 0.24604141291108406\n",
      "fog_index: 9.220638787386656\n",
      "avg_no_of_words: 22.805555555555557\n",
      "complex_words: 202\n",
      "word_count: 448\n",
      "syllable_count_per_word: 1.633373934226553\n",
      "personal_pronouns: 2\n",
      "avg_word_length: 5.953168044077135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-machine-learning-will-affect-your-business/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('45.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"45.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"45.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd1b4ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 108\n",
      "neg_score: 109\n",
      "polarity: -0.004608294909639194\n",
      "subjectivity: 0.2589498803592484\n",
      "avg_sent_length: 29.82716049382716\n",
      "percentage_complex 0.2591059602649007\n",
      "fog_index: 12.034506581636826\n",
      "avg_no_of_words: 29.82716049382716\n",
      "complex_words: 626\n",
      "word_count: 1265\n",
      "syllable_count_per_word: 1.7471026490066226\n",
      "personal_pronouns: 11\n",
      "avg_word_length: 6.051460361613352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/deep-learning-impact-on-areas-of-e-learning/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('46.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"46.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"46.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ece438c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 92\n",
      "neg_score: 124\n",
      "polarity: -0.14814814746227709\n",
      "subjectivity: 0.22954303907593726\n",
      "avg_sent_length: 23.048076923076923\n",
      "percentage_complex 0.2907801418439716\n",
      "fog_index: 9.335542825968359\n",
      "avg_no_of_words: 23.048076923076923\n",
      "complex_words: 697\n",
      "word_count: 1393\n",
      "syllable_count_per_word: 1.640383813099708\n",
      "personal_pronouns: 1\n",
      "avg_word_length: 6.34919028340081\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-to-protect-future-data-and-its-privacy-blackcoffer/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('47.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"47.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"47.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02edeb27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 59\n",
      "neg_score: 47\n",
      "polarity: 0.1132075461018156\n",
      "subjectivity: 0.19850187228744967\n",
      "avg_sent_length: 22.551724137931036\n",
      "percentage_complex 0.308868501529052\n",
      "fog_index: 9.144237055784036\n",
      "avg_no_of_words: 22.551724137931036\n",
      "complex_words: 404\n",
      "word_count: 748\n",
      "syllable_count_per_word: 1.9357798165137614\n",
      "personal_pronouns: 10\n",
      "avg_word_length: 6.621530698065602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-machines-ai-automations-and-robo-human-are-effective-in-finance-and-banking/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('48.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"48.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"48.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fa05d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 63\n",
      "neg_score: 65\n",
      "polarity: -0.01562499987792969\n",
      "subjectivity: 0.23529411721453286\n",
      "avg_sent_length: 17.414634146341463\n",
      "percentage_complex 0.2927170868347339\n",
      "fog_index: 7.082940493270479\n",
      "avg_no_of_words: 17.414634146341463\n",
      "complex_words: 418\n",
      "word_count: 839\n",
      "syllable_count_per_word: 1.673669467787115\n",
      "personal_pronouns: 6\n",
      "avg_word_length: 6.178861788617886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/ai-human-robotics-machine-future-planet-blackcoffer-thinking-jobs-workplace/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('49.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"49.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"49.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77d39e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 91\n",
      "neg_score: 71\n",
      "polarity: 0.12345678936137784\n",
      "subjectivity: 0.26999999955\n",
      "avg_sent_length: 29.253968253968253\n",
      "percentage_complex 0.24579489962018447\n",
      "fog_index: 11.799905261435375\n",
      "avg_no_of_words: 29.253968253968253\n",
      "complex_words: 453\n",
      "word_count: 894\n",
      "syllable_count_per_word: 1.7585458491589798\n",
      "personal_pronouns: 33\n",
      "avg_word_length: 5.99645180366647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-ai-will-change-the-world-blackcoffer/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('50.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"50.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"50.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fee0928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 92\n",
      "neg_score: 84\n",
      "polarity: 0.04545454519628099\n",
      "subjectivity: 0.2674772032409161\n",
      "avg_sent_length: 22.53012048192771\n",
      "percentage_complex 0.28449197860962566\n",
      "fog_index: 9.125844984214934\n",
      "avg_no_of_words: 22.53012048192771\n",
      "complex_words: 532\n",
      "word_count: 1016\n",
      "syllable_count_per_word: 1.750802139037433\n",
      "personal_pronouns: 11\n",
      "avg_word_length: 6.248170731707317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/future-of-work-how-ai-has-entered-the-workplace/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('51.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"51.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"51.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "725d1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 31\n",
      "neg_score: 35\n",
      "polarity: -0.06060605968778698\n",
      "subjectivity: 0.2894736829409049\n",
      "avg_sent_length: 25.636363636363637\n",
      "percentage_complex 0.3067375886524823\n",
      "fog_index: 10.377240490006448\n",
      "avg_no_of_words: 25.636363636363637\n",
      "complex_words: 173\n",
      "word_count: 317\n",
      "syllable_count_per_word: 1.9166666666666667\n",
      "personal_pronouns: 0\n",
      "avg_word_length: 6.569444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/ai-tool-alexa-google-assistant-finance-banking-tool-future/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('52.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"52.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"52.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f6bf4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 113\n",
      "neg_score: 99\n",
      "polarity: 0.06603773553755785\n",
      "subjectivity: 0.3475409830368181\n",
      "avg_sent_length: 210.55555555555554\n",
      "percentage_complex 0.21846965699208443\n",
      "fog_index: 84.30961008501906\n",
      "avg_no_of_words: 210.55555555555554\n",
      "complex_words: 414\n",
      "word_count: 940\n",
      "syllable_count_per_word: 1.763060686015831\n",
      "personal_pronouns: 16\n",
      "avg_word_length: 5.829646017699115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/ai-healthcare-revolution-ml-technology-algorithm-google-analytics-industrialrevolution/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('53.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"53.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"53.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be082ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 49\n",
      "neg_score: 58\n",
      "polarity: -0.08411214874661543\n",
      "subjectivity: 0.27295918297714494\n",
      "avg_sent_length: 19.285714285714285\n",
      "percentage_complex 0.2972222222222222\n",
      "fog_index: 7.833174603174602\n",
      "avg_no_of_words: 19.285714285714285\n",
      "complex_words: 321\n",
      "word_count: 607\n",
      "syllable_count_per_word: 1.7722222222222221\n",
      "personal_pronouns: 0\n",
      "avg_word_length: 6.321129707112971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/all-you-need-to-know-about-online-marketing/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('54.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"54.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"54.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92189ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 39\n",
      "neg_score: 43\n",
      "polarity: -0.04878048720999406\n",
      "subjectivity: 0.24924012082297836\n",
      "avg_sent_length: 20.073170731707318\n",
      "percentage_complex 0.32563791008505466\n",
      "fog_index: 8.159523456716949\n",
      "avg_no_of_words: 20.073170731707318\n",
      "complex_words: 268\n",
      "word_count: 467\n",
      "syllable_count_per_word: 1.8177399756986634\n",
      "personal_pronouns: 1\n",
      "avg_word_length: 6.644444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/evolution-of-advertising-industry/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('55.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"55.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"55.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "804b6a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 8\n",
      "neg_score: 16\n",
      "polarity: -0.333333319444445\n",
      "subjectivity: 0.18749999853515625\n",
      "avg_sent_length: 29.416666666666668\n",
      "percentage_complex 0.32011331444759206\n",
      "fog_index: 11.894711992445705\n",
      "avg_no_of_words: 29.416666666666668\n",
      "complex_words: 113\n",
      "word_count: 203\n",
      "syllable_count_per_word: 1.8130311614730878\n",
      "personal_pronouns: 2\n",
      "avg_word_length: 6.542857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-data-analytics-can-help-your-business-respond-to-the-impact-of-covid-19/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('56.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"56.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"56.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a84b5747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 57\n",
      "neg_score: 115\n",
      "polarity: -0.3372093003650622\n",
      "subjectivity: 0.30659536487237904\n",
      "avg_sent_length: 27.303571428571427\n",
      "percentage_complex 0.27207325049051667\n",
      "fog_index: 11.030257871624778\n",
      "avg_no_of_words: 27.303571428571427\n",
      "complex_words: 416\n",
      "word_count: 842\n",
      "syllable_count_per_word: 1.788750817527796\n",
      "personal_pronouns: 2\n",
      "avg_word_length: 6.2327272727272724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/environmental-impact-of-the-covid-19-pandemic-lesson-for-the-future/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('58.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"58.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"58.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f5ede46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 35\n",
      "neg_score: 45\n",
      "polarity: -0.12499999843750002\n",
      "subjectivity: 0.29520295094021054\n",
      "avg_sent_length: 27.655172413793103\n",
      "percentage_complex 0.2743142144638404\n",
      "fog_index: 11.171794651302777\n",
      "avg_no_of_words: 27.655172413793103\n",
      "complex_words: 220\n",
      "word_count: 430\n",
      "syllable_count_per_word: 1.7119700748129676\n",
      "personal_pronouns: 5\n",
      "avg_word_length: 6.098611111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-data-analytics-and-ai-are-used-to-halt-the-covid-19-pandemic/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('59.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"59.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"59.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22aa61f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 121\n",
      "neg_score: 137\n",
      "polarity: -0.06201550363559882\n",
      "subjectivity: 0.36857142804489795\n",
      "avg_sent_length: 19.63063063063063\n",
      "percentage_complex 0.25011473152822394\n",
      "fog_index: 7.952298144863542\n",
      "avg_no_of_words: 19.63063063063063\n",
      "complex_words: 545\n",
      "word_count: 1102\n",
      "syllable_count_per_word: 1.671867829279486\n",
      "personal_pronouns: 10\n",
      "avg_word_length: 6.017071908949819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/difference-between-artificial-intelligence-machine-learning-statistics-and-data-mining/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('60.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"60.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"60.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d0847a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 46\n",
      "neg_score: 35\n",
      "polarity: 0.1358024674592288\n",
      "subjectivity: 0.27457627025567366\n",
      "avg_sent_length: 17.386363636363637\n",
      "percentage_complex 0.29411764705882354\n",
      "fog_index: 7.072192513368984\n",
      "avg_no_of_words: 17.386363636363637\n",
      "complex_words: 225\n",
      "word_count: 434\n",
      "syllable_count_per_word: 1.7163398692810456\n",
      "personal_pronouns: 4\n",
      "avg_word_length: 6.284226190476191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-python-became-the-first-choice-for-data-science/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('61.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"61.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"61.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "688d8643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 20\n",
      "neg_score: 19\n",
      "polarity: 0.025641024983563465\n",
      "subjectivity: 0.24840764172988763\n",
      "avg_sent_length: 24.705882352941178\n",
      "percentage_complex 0.2261904761904762\n",
      "fog_index: 9.972829131652661\n",
      "avg_no_of_words: 24.705882352941178\n",
      "complex_words: 95\n",
      "word_count: 251\n",
      "syllable_count_per_word: 1.7357142857142858\n",
      "personal_pronouns: 0\n",
      "avg_word_length: 6.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-google-fit-measure-heart-and-respiratory-rates-using-a-phone/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('62.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"62.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"62.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6d7625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 45\n",
      "neg_score: 35\n",
      "polarity: 0.12499999843750002\n",
      "subjectivity: 0.18648018604550073\n",
      "avg_sent_length: 26.20408163265306\n",
      "percentage_complex 0.26479750778816197\n",
      "fog_index: 10.58755165617649\n",
      "avg_no_of_words: 26.20408163265306\n",
      "complex_words: 340\n",
      "word_count: 728\n",
      "syllable_count_per_word: 1.6565420560747663\n",
      "personal_pronouns: 4\n",
      "avg_word_length: 5.994732221246708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/what-is-the-future-of-mobile-apps/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('63.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"63.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"63.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "275d7e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 50\n",
      "neg_score: 50\n",
      "polarity: 0.0\n",
      "subjectivity: 0.34013605326484336\n",
      "avg_sent_length: 29.0\n",
      "percentage_complex 0.3175287356321839\n",
      "fog_index: 11.727011494252874\n",
      "avg_no_of_words: 29.0\n",
      "complex_words: 221\n",
      "word_count: 414\n",
      "syllable_count_per_word: 1.8922413793103448\n",
      "personal_pronouns: 1\n",
      "avg_word_length: 6.640650406504065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-ai-in-health-and-medicine/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('64.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"64.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"64.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "276ede0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 20\n",
      "neg_score: 25\n",
      "polarity: -0.11111110864197538\n",
      "subjectivity: 0.335820893016262\n",
      "avg_sent_length: 15.714285714285714\n",
      "percentage_complex 0.225\n",
      "fog_index: 6.3757142857142854\n",
      "avg_no_of_words: 15.714285714285714\n",
      "complex_words: 99\n",
      "word_count: 219\n",
      "syllable_count_per_word: 1.768181818181818\n",
      "personal_pronouns: 0\n",
      "avg_word_length: 6.268041237113402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/telemedicine-what-patients-like-and-dislike-about-it/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('65.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"65.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"65.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71934603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 15\n",
      "neg_score: 12\n",
      "polarity: 0.11111110699588492\n",
      "subjectivity: 0.37499999479166674\n",
      "avg_sent_length: 16.545454545454547\n",
      "percentage_complex 0.33516483516483514\n",
      "fog_index: 6.7522477522477535\n",
      "avg_no_of_words: 16.545454545454547\n",
      "complex_words: 61\n",
      "word_count: 110\n",
      "syllable_count_per_word: 1.8516483516483517\n",
      "personal_pronouns: 1\n",
      "avg_word_length: 6.5030674846625764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-we-forecast-future-technologies/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('66.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"66.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"66.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30d30dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 46\n",
      "neg_score: 87\n",
      "polarity: -0.3082706743739047\n",
      "subjectivity: 0.48717948539494693\n",
      "avg_sent_length: 16.964285714285715\n",
      "percentage_complex 0.22526315789473683\n",
      "fog_index: 6.875819548872181\n",
      "avg_no_of_words: 16.964285714285715\n",
      "complex_words: 214\n",
      "word_count: 478\n",
      "syllable_count_per_word: 1.5568421052631578\n",
      "personal_pronouns: 19\n",
      "avg_word_length: 5.897022332506204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/can-robots-tackle-late-life-loneliness/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('67.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "file_content = open(\"67.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"67.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cffc9626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 9\n",
      "neg_score: 11\n",
      "polarity: -0.09999999500000024\n",
      "subjectivity: 0.32258063995837677\n",
      "avg_sent_length: 20.3\n",
      "percentage_complex 0.22660098522167488\n",
      "fog_index: 8.21064039408867\n",
      "avg_no_of_words: 20.3\n",
      "complex_words: 46\n",
      "word_count: 112\n",
      "syllable_count_per_word: 1.6551724137931034\n",
      "personal_pronouns: 1\n",
      "avg_word_length: 5.861878453038674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/embedding-care-robots-into-society-socio-technical-considerations/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('68.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"68.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"68.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "038d79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 74\n",
      "neg_score: 88\n",
      "polarity: -0.0864197525529645\n",
      "subjectivity: 0.2950819666756248\n",
      "avg_sent_length: 21.983333333333334\n",
      "percentage_complex 0.3260045489006823\n",
      "fog_index: 8.923735152893606\n",
      "avg_no_of_words: 21.983333333333334\n",
      "complex_words: 430\n",
      "word_count: 781\n",
      "syllable_count_per_word: 1.9408642911296436\n",
      "personal_pronouns: 0\n",
      "avg_word_length: 6.802415875754961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/management-challenges-for-future-digitalization-of-healthcare-services/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('69.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"69.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"69.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1905867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 21\n",
      "neg_score: 38\n",
      "polarity: -0.28813558833668496\n",
      "subjectivity: 0.3025641010124918\n",
      "avg_sent_length: 29.65\n",
      "percentage_complex 0.24957841483979765\n",
      "fog_index: 11.95983136593592\n",
      "avg_no_of_words: 29.65\n",
      "complex_words: 148\n",
      "word_count: 336\n",
      "syllable_count_per_word: 1.7133220910623945\n",
      "personal_pronouns: 10\n",
      "avg_word_length: 6.204545454545454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/are-we-any-closer-to-preventing-a-nuclear-holocaust/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('70.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "file_content = open(\"70.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"70.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c062bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 44\n",
      "neg_score: 77\n",
      "polarity: -0.27272727047332834\n",
      "subjectivity: 0.31185566929934105\n",
      "avg_sent_length: 22.568627450980394\n",
      "percentage_complex 0.23023457862728064\n",
      "fog_index: 9.11954481184307\n",
      "avg_no_of_words: 22.568627450980394\n",
      "complex_words: 265\n",
      "word_count: 620\n",
      "syllable_count_per_word: 1.7028670721112076\n",
      "personal_pronouns: 7\n",
      "avg_word_length: 5.931132075471698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/will-technology-eliminate-the-need-for-animal-testing-in-drug-development/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('71.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"71.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"71.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89a146a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 62\n",
      "neg_score: 123\n",
      "polarity: -0.3297297279474069\n",
      "subjectivity: 0.26241134714551584\n",
      "avg_sent_length: 26.716216216216218\n",
      "percentage_complex 0.28426909458775923\n",
      "fog_index: 10.800194124321592\n",
      "avg_no_of_words: 26.716216216216218\n",
      "complex_words: 562\n",
      "word_count: 1072\n",
      "syllable_count_per_word: 1.8012139605462822\n",
      "personal_pronouns: 15\n",
      "avg_word_length: 6.5037768739105175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/will-we-ever-understand-the-nature-of-consciousness/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('72.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"72.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"72.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40a03479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 95\n",
      "neg_score: 94\n",
      "polarity: 0.005291005263010554\n",
      "subjectivity: 0.3321616865867106\n",
      "avg_sent_length: 27.681159420289855\n",
      "percentage_complex 0.2089005235602094\n",
      "fog_index: 11.156023977540027\n",
      "avg_no_of_words: 27.681159420289855\n",
      "complex_words: 399\n",
      "word_count: 963\n",
      "syllable_count_per_word: 1.6712041884816753\n",
      "personal_pronouns: 19\n",
      "avg_word_length: 5.827822120866591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/will-we-ever-colonize-outer-space/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('73.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"73.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"73.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d89c81b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 63\n",
      "neg_score: 60\n",
      "polarity: 0.02439024370414436\n",
      "subjectivity: 0.22082585238630906\n",
      "avg_sent_length: 20.365853658536587\n",
      "percentage_complex 0.2622754491017964\n",
      "fog_index: 8.251251643055353\n",
      "avg_no_of_words: 20.365853658536587\n",
      "complex_words: 438\n",
      "word_count: 892\n",
      "syllable_count_per_word: 1.6137724550898203\n",
      "personal_pronouns: 18\n",
      "avg_word_length: 5.999310820124053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/what-is-the-chance-homo-sapiens-will-survive-for-the-next-500-years/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('74.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"74.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"74.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd720394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 48\n",
      "neg_score: 53\n",
      "polarity: -0.04950495000490149\n",
      "subjectivity: 0.34707903660797584\n",
      "avg_sent_length: 16.06451612903226\n",
      "percentage_complex 0.24397590361445784\n",
      "fog_index: 6.523396813058688\n",
      "avg_no_of_words: 16.06451612903226\n",
      "complex_words: 243\n",
      "word_count: 469\n",
      "syllable_count_per_word: 1.5441767068273093\n",
      "personal_pronouns: 1\n",
      "avg_word_length: 5.711211778029445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/why-does-your-business-need-a-chatbot/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('75.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"75.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"75.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb446098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 49\n",
      "neg_score: 50\n",
      "polarity: -0.010101009998979697\n",
      "subjectivity: 0.22863741286688818\n",
      "avg_sent_length: 24.861538461538462\n",
      "percentage_complex 0.2079207920792079\n",
      "fog_index: 10.027783701447069\n",
      "avg_no_of_words: 24.861538461538462\n",
      "complex_words: 336\n",
      "word_count: 764\n",
      "syllable_count_per_word: 1.5309405940594059\n",
      "personal_pronouns: 4\n",
      "avg_word_length: 5.699290780141844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-you-lead-a-project-or-a-team-without-any-technical-expertise/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('76.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"76.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"76.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f65e2632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 40\n",
      "neg_score: 36\n",
      "polarity: 0.052631578254847655\n",
      "subjectivity: 0.4935064903019059\n",
      "avg_sent_length: 32.9375\n",
      "percentage_complex 0.2409867172675522\n",
      "fog_index: 13.271394686907023\n",
      "avg_no_of_words: 32.9375\n",
      "complex_words: 127\n",
      "word_count: 245\n",
      "syllable_count_per_word: 1.6793168880455407\n",
      "personal_pronouns: 8\n",
      "avg_word_length: 5.799145299145299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/can-you-be-great-leader-without-technical-expertise/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('77.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"77.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"77.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e0f9bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 84\n",
      "neg_score: 76\n",
      "polarity: 0.049999999687500005\n",
      "subjectivity: 0.2420574882873563\n",
      "avg_sent_length: 25.380281690140844\n",
      "percentage_complex 0.290788013318535\n",
      "fog_index: 10.268427881383753\n",
      "avg_no_of_words: 25.380281690140844\n",
      "complex_words: 524\n",
      "word_count: 994\n",
      "syllable_count_per_word: 1.785793562708102\n",
      "personal_pronouns: 8\n",
      "avg_word_length: 6.275753768844221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-does-artificial-intelligence-affect-the-environment/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('78.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"78.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"78.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48ce5a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 34\n",
      "neg_score: 104\n",
      "polarity: -0.5072463731358958\n",
      "subjectivity: 0.39884392948311004\n",
      "avg_sent_length: 15.077922077922079\n",
      "percentage_complex 0.23944875107665806\n",
      "fog_index: 6.126948331599495\n",
      "avg_no_of_words: 15.077922077922079\n",
      "complex_words: 278\n",
      "word_count: 597\n",
      "syllable_count_per_word: 1.6210163652024117\n",
      "personal_pronouns: 13\n",
      "avg_word_length: 5.931243680485339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('79.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"79.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"79.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "daf32c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 78\n",
      "neg_score: 107\n",
      "polarity: -0.15675675590942295\n",
      "subjectivity: 0.48302871936546027\n",
      "avg_sent_length: 16.727272727272727\n",
      "percentage_complex 0.20652173913043478\n",
      "fog_index: 6.773517786561264\n",
      "avg_no_of_words: 16.727272727272727\n",
      "complex_words: 266\n",
      "word_count: 659\n",
      "syllable_count_per_word: 1.4922360248447204\n",
      "personal_pronouns: 5\n",
      "avg_word_length: 5.668145519077196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/is-perfection-the-greatest-enemy-of-productivity/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('80.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"80.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"80.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f37e3816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 54\n",
      "neg_score: 105\n",
      "polarity: -0.3207547149638068\n",
      "subjectivity: 0.31927710779261625\n",
      "avg_sent_length: 28.543478260869566\n",
      "percentage_complex 0.23076923076923078\n",
      "fog_index: 11.50969899665552\n",
      "avg_no_of_words: 28.543478260869566\n",
      "complex_words: 303\n",
      "word_count: 800\n",
      "syllable_count_per_word: 1.654988575780655\n",
      "personal_pronouns: 6\n",
      "avg_word_length: 6.063811922753988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/global-financial-crisis-2008-causes-effects-and-its-solution/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('81.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"81.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"81.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe3cd63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 51\n",
      "neg_score: 95\n",
      "polarity: -0.3013698609495215\n",
      "subjectivity: 0.23101265786232175\n",
      "avg_sent_length: 20.21951219512195\n",
      "percentage_complex 0.27744270205066346\n",
      "fog_index: 8.198781958869047\n",
      "avg_no_of_words: 20.21951219512195\n",
      "complex_words: 460\n",
      "word_count: 978\n",
      "syllable_count_per_word: 1.7322074788902293\n",
      "personal_pronouns: 10\n",
      "avg_word_length: 6.3707317073170735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/gender-diversity-and-equality-in-the-tech-industry/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('82.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"82.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"82.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "302e4d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 35\n",
      "neg_score: 115\n",
      "polarity: -0.5333333297777778\n",
      "subjectivity: 0.4054054043097151\n",
      "avg_sent_length: 15.39240506329114\n",
      "percentage_complex 0.2450657894736842\n",
      "fog_index: 6.25498834110593\n",
      "avg_no_of_words: 15.39240506329114\n",
      "complex_words: 298\n",
      "word_count: 627\n",
      "syllable_count_per_word: 1.6373355263157894\n",
      "personal_pronouns: 15\n",
      "avg_word_length: 5.984615384615385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('83.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"83.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"83.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c540adb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 74\n",
      "neg_score: 91\n",
      "polarity: -0.10303030240587696\n",
      "subjectivity: 0.2967625893943119\n",
      "avg_sent_length: 32.074074074074076\n",
      "percentage_complex 0.24422632794457275\n",
      "fog_index: 12.927320160807461\n",
      "avg_no_of_words: 32.074074074074076\n",
      "complex_words: 423\n",
      "word_count: 923\n",
      "syllable_count_per_word: 1.632217090069284\n",
      "personal_pronouns: 5\n",
      "avg_word_length: 5.904309252217997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-small-business-can-survive-the-coronavirus-crisis/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('84.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"84.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"84.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70ca81be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url =\"https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-tourism-aviation-industries/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('87.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"87.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"87.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-sports-events-around-the-world/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('88.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"88.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"88.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/changing-landscape-and-emerging-trends-in-the-indian-it-ites-industry/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('89.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"89.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"89.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83545907",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/online-gaming-adolescent-online-gaming-effects-demotivated-depression-musculoskeletal-and-psychosomatic-symptoms/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('90.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"90.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"90.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e1ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/human-rights-outlook/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('91.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"91.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"91.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-voice-search-makes-your-business-a-successful-business/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('92.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"92.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"92.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ad2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-the-covid-19-crisis-is-redefining-jobs-and-services/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('93.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"93.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"93.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b40c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-to-increase-social-media-engagement-for-marketers/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('94.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"94.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"94.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357808a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('96.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"96.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"96.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-5/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('97.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"97.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"97.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3dc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-4/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('98.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"98.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"98.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8af947",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('99.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"99.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"99.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c51700",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-3/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('100.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"100.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"100.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/travel-and-tourism-outlook/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('101.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"101.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"101.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/gaming-disorder-and-effects-of-gaming-on-health/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('102.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"102.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"102.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39251435",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('103.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"103.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"103.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4918b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('104.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"104.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"104.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-office-space-and-co-working-industries/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('105.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"105.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"105.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97f53f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/contribution-of-handicrafts-visual-arts-literature-in-the-indian-economy/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('106.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"106.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"106.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-covid-19-is-impacting-payment-preferences/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('107.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"107.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"107.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('108.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"108.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"108.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8567fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('109.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"109.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"109.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/covid-19-how-have-countries-been-responding/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('110.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"110.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"110.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('111.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"111.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"111.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ef332b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-3/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('112.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"112.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"112.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-3/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('113.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"113.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"113.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eaa2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('114.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"114.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"114.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/covid-19-how-have-countries-been-responding-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('115.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"115.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"115.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44128b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-4/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('116.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"116.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"116.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54a9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('117.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"117.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"117.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c2075e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-3/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('118.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"118.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"118.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39842bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-4/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('119.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"119.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"119.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/why-scams-like-nirav-modi-happen-with-indian-banks/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('120.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"120.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"120.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19-on-the-global-economy/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('121.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"121.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"121.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e87a72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19coronavirus-on-the-indian-economy-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('122.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"122.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"122.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76369d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19-on-the-global-economy-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('123.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"123.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"123.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d23b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy-3/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('124.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"124.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"124.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/should-celebrities-be-allowed-to-join-politics/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('125.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"125.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"125.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c21f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-prepared-is-india-to-tackle-a-possible-covid-19-outbreak/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('126.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"126.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"126.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "693f5468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('127.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"127.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"127.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9eb8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/controversy-as-a-marketing-strategy/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('128.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"128.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"128.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a484b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('129.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"129.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"129.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fca5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('130.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"130.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"130.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea425e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/what-are-the-key-policies-that-will-mitigate-the-impacts-of-covid-19-on-the-world-of-work/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('131.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"131.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"131.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7702a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/marketing-drives-results-with-a-focus-on-problems/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('132.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"132.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"132.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fce2629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/continued-demand-for-sustainability/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('133.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"133.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"133.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2facbd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/coronavirus-disease-covid-19-effect-the-impact-and-role-of-mass-media-during-the-pandemic/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('134.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"134.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"134.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/should-people-wear-fabric-gloves-seeking-evidence-regarding-the-differential-transfer-of-covid-19-or-coronaviruses-generally-between-surfaces/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('135.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"135.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"135.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcf4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/why-is-there-a-severe-immunological-and-inflammatory-explosion-in-those-affected-by-sarms-covid-19/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('136.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"136.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"136.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d764c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/what-do-you-think-is-the-lesson-or-lessons-to-be-learned-with-covid-19/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('137.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"137.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"137.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/coronavirus-the-unexpected-challenge-for-the-european-union/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('138.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"138.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"138.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/industrial-revolution-4-0-pros-and-cons/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('139.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"139.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"139.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8e97dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_score: 45\n",
      "neg_score: 97\n",
      "polarity: -0.36619718051973815\n",
      "subjectivity: 0.337292160719021\n",
      "avg_sent_length: 27.975609756097562\n",
      "percentage_complex 0.26242371403661724\n",
      "fog_index: 11.295213388053673\n",
      "avg_no_of_words: 27.975609756097562\n",
      "complex_words: 301\n",
      "word_count: 672\n",
      "syllable_count_per_word: 1.6904969485614647\n",
      "personal_pronouns: 0\n",
      "avg_word_length: 6.114313160422671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('140.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"140.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"140.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399dd535",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy-2/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('141.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"141.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"141.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19coronavirus-on-the-indian-economy/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('142.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"142.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"142.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ccba4d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_sent_length1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m avg_sent_length \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     50\u001b[0m percentage_complex \u001b[38;5;241m=\u001b[39m complex_words \u001b[38;5;241m/\u001b[39m no_of_words \n\u001b[1;32m---> 51\u001b[0m fog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mavg_sent_length1\u001b[49m \u001b[38;5;241m+\u001b[39m percentage_complex)\n\u001b[0;32m     52\u001b[0m avg_no_of_words \u001b[38;5;241m=\u001b[39m no_of_words \u001b[38;5;241m/\u001b[39m total_sentence\n\u001b[0;32m     54\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_sent_length1' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-global-economy/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('143.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"143.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"143.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/blockchain-in-fintech/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('145.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"145.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"145.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/blockchain-for-payments/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('146.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"146.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"146.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/the-future-of-investing/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('147.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"147.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"147.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/big-data-analytics-in-healthcare/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('148.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"148.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"148.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86001add",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/business-analytics-in-the-healthcare-industry/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('149.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"149.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"149.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://insights.blackcoffer.com/challenges-and-opportunities-of-big-data-in-healthcare/\"\n",
    "raw_request = Request(url)\n",
    "raw_request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0')\n",
    "raw_request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n",
    "resp = urlopen(raw_request)\n",
    "raw_html = resp.read()\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "blog_text = soup.find('div', class_='td-post-content').text\n",
    "file = open('150.txt', 'w',encoding ='utf-8')\n",
    "file.write(blog_text.strip())\n",
    "file.close()\n",
    "\n",
    "file_content = open(\"150.txt\",encoding=\"utf8\").read().upper()\n",
    "tokenized_word = word_tokenize(file_content)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "custom_stopword = str(open(\"Stopwords.txt\",encoding=\"utf8\").read().split()).upper()\n",
    "filtered =[]\n",
    "for w in tokenized_word:\n",
    "    if w not in custom_stopword:\n",
    "        filtered.append(w) \n",
    "        \n",
    "tokenized_text = sent_tokenize(file_content)\n",
    "filtered_sent =[]\n",
    "for i in tokenized_text:\n",
    "    if i not in custom_stopword:\n",
    "        filtered_sent.append(i)\n",
    "        \n",
    "file = open('negative.txt', 'r')\n",
    "neg_words = str(file.read().split()).upper()\n",
    "file = open('positive.txt', 'r')\n",
    "pos_words = str(file.read().split()).upper()\n",
    "total_words = len(filtered)\n",
    "pos_score = len([w for w in filtered if w in pos_words])\n",
    "neg_score = len([w for w in filtered if w in neg_words])\n",
    "polarity = (pos_score -neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "subjectivity = (pos_score + neg_score)/ ((total_words) + 0.000001)\n",
    "\n",
    "total_sentence = len(tokenized_text)\n",
    "no_of_words = len(tokenized_word)\n",
    "\n",
    "def syllables():\n",
    "    count = len(file_content.split())\n",
    "    for i in file_content.split():\n",
    "        if re.match(r'^[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]*[BCDFGHJKLMNPQRSTVWXYZ]*[AEIOU]+[BCDFGHJKLMNPQRSTVWXYZ]*$', i):\n",
    "            count -= 1\n",
    "    return count \n",
    "complex_words = syllables()\n",
    "avg_sent_length = no_of_words / total_sentence\n",
    "percentage_complex = complex_words / no_of_words \n",
    "fog_index = 0.4 * (avg_sent_length1 + percentage_complex)\n",
    "avg_no_of_words = no_of_words / total_sentence\n",
    "\n",
    "stop_word =set(stopwords.words(\"english\"))\n",
    "stop_words = str(stop_word).upper()\n",
    "word_c=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        word_c.append(w)\n",
    "        \n",
    "word = str(word_c).replace('?','').replace('.','').replace(',','').replace('!','').split()\n",
    "word_count = len(word)\n",
    "\n",
    "string = file_content\n",
    "vowels = \"AaEeIiOoUu\"\n",
    "final = [each for each in string if each in vowels]\n",
    "no_of_vowels = len(final)\n",
    "syllable_count_per_word = no_of_vowels / no_of_words\n",
    "\n",
    "pronouns = pronounRegex.findall(open(\"150.txt\",encoding=\"utf8\").read().lower())\n",
    "personal_pronouns = len(pronouns)\n",
    "\n",
    "avg_word_length = len(file_content) / len(file_content.split())\n",
    "\n",
    "print(\"pos_score:\",pos_score)\n",
    "print(\"neg_score:\",neg_score)\n",
    "print(\"polarity:\",polarity)\n",
    "print(\"subjectivity:\",subjectivity)\n",
    "print(\"avg_sent_length:\",avg_sent_length)\n",
    "print(\"percentage_complex\",percentage_complex)\n",
    "print(\"fog_index:\",fog_index)\n",
    "print(\"avg_no_of_words:\",avg_no_of_words)\n",
    "print(\"complex_words:\",complex_words)\n",
    "print(\"word_count:\",word_count)\n",
    "print(\"syllable_count_per_word:\",syllable_count_per_word)\n",
    "print(\"personal_pronouns:\",personal_pronouns)\n",
    "print(\"avg_word_length:\",avg_word_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
